{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning / splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after english_cleaners : hello world !\n",
      "Text after collapse_whitespace : Hello World !\n",
      "Text after lowercase : hello   world    !\n",
      "\n",
      "Text after french_cleaners : bonjour a tous !\n",
      "Text after convert_to_ascii : Bonjour  a Tous   !\n",
      "Text after fr_convert_to_ascii : Bonjour  a Tous   !\n"
     ]
    }
   ],
   "source": [
    "from utils.text.cleaners import french_cleaners, english_cleaners\n",
    "from utils.text.cleaners import expand_numbers, collapse_whitespace, fr_convert_to_ascii, convert_to_ascii, lowercase\n",
    "\n",
    "en_text = \"Hello   World    !\"\n",
    "fr_text = \"Bonjour  à Tous   !\"\n",
    "\n",
    "print(\"Text after english_cleaners : {}\".format(english_cleaners(en_text)))\n",
    "print(\"Text after collapse_whitespace : {}\".format(collapse_whitespace(en_text)))\n",
    "print(\"Text after lowercase : {}\".format(lowercase(en_text)))\n",
    "print()\n",
    "print(\"Text after french_cleaners : {}\".format(french_cleaners(fr_text)))\n",
    "print(\"Text after convert_to_ascii : {}\".format(convert_to_ascii(fr_text)))\n",
    "print(\"Text after fr_convert_to_ascii : {}\".format(fr_convert_to_ascii(fr_text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text part 0 (length = 124) : Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore  et dolore magna aliqua.\n",
      "Text part 1 (length = 108) : Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut  aliquip ex ea commodo consequat.\n",
      "Text part 2 (length = 103) : Duis aute irure dolor in reprehenderit in voluptate velit esse cillum  dolore eu fugiat nulla pariatur.\n",
      "Text part 3 (length = 111) : Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia  deserunt mollit anim id est laborum.\n"
     ]
    }
   ],
   "source": [
    "from utils.text import split_text\n",
    "\n",
    "long_text = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore \n",
    "et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut \n",
    "aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum \n",
    "dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia \n",
    "deserunt mollit anim id est laborum.\"\"\"\n",
    "for i, p in enumerate(split_text(long_text)):\n",
    "    print(\"Text part {} (length = {}) : {}\".format(i, len(p), p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a text encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a Text encoder\n",
    "\n",
    "You can also provide a `vocab_size` higher than the actual number of tokens : it will create `ukn_x` tokens. It can be useful if you need a fixed `vocab_size` or if you would like to have the possibility to add tokens in the future. \n",
    "\n",
    "It is the way I use it in the Tacotron-2 model because the English version has a `vocab_size` of 148 while the French has less characters (and thus a smaller vocabulary). \n",
    "\n",
    "You can also initialize a `TextEncoder` based on a `transformers` pretrained : `BERT`, `BART`, `MBart` and `GPT-2` are currently supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using token / word-level tokenizer, it can be useful to add 'detach_punctuation' in cleaners\n",
      "========== Text encoder ==========\n",
      "Vocab (size = 70) : ['_', '-', '!', \"'\", '(', ')', ',', '.', ':', ';', '?', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l']\n",
      "Config : {\n",
      "  \"level\": 0,\n",
      "  \"lstrip\": false,\n",
      "  \"rstrip\": false,\n",
      "  \"cleaners\": [\n",
      "    \"french_cleaners\"\n",
      "  ],\n",
      "  \"split_pattern\": null,\n",
      "  \"bpe_end_of_word\": null,\n",
      "  \"pad_token\": \"\",\n",
      "  \"sep_token\": null,\n",
      "  \"ukn_token\": null,\n",
      "  \"sos_token\": \"[SOS]\",\n",
      "  \"eos_token\": \"[EOS]\",\n",
      "  \"mask_token\": null,\n",
      "  \"sub_word_prefix\": \"\",\n",
      "  \"use_sos_and_eos\": false\n",
      "}\n",
      "\n",
      "========== Text encoder ==========\n",
      "Vocab (size = 148) : ['_', '-', '!', \"'\", '(', ')', ',', '.', ':', ';', '?', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l']\n",
      "Config : {\n",
      "  \"level\": 0,\n",
      "  \"lstrip\": false,\n",
      "  \"rstrip\": false,\n",
      "  \"cleaners\": [\n",
      "    {\n",
      "      \"name\": \"french_cleaners\",\n",
      "      \"to_lowercase\": false\n",
      "    }\n",
      "  ],\n",
      "  \"split_pattern\": null,\n",
      "  \"bpe_end_of_word\": null,\n",
      "  \"pad_token\": \"\",\n",
      "  \"sep_token\": null,\n",
      "  \"ukn_token\": null,\n",
      "  \"sos_token\": \"[SOS]\",\n",
      "  \"eos_token\": \"[EOS]\",\n",
      "  \"mask_token\": null,\n",
      "  \"sub_word_prefix\": \"\",\n",
      "  \"use_sos_and_eos\": false\n",
      "}\n",
      "\n",
      "========== Text encoder ==========\n",
      "Vocab (size = 50257) : ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R']\n",
      "Config : {\n",
      "  \"level\": 1,\n",
      "  \"lstrip\": false,\n",
      "  \"rstrip\": true,\n",
      "  \"cleaners\": [],\n",
      "  \"split_pattern\": \"'s|'t|'re|'ve|'m|'ll|'d| ?\\\\p{L}+| ?\\\\p{N}+| ?[^\\\\s\\\\p{L}\\\\p{N}]+|\\\\s+(?!\\\\S)|\\\\s+\",\n",
      "  \"bpe_end_of_word\": null,\n",
      "  \"pad_token\": null,\n",
      "  \"sep_token\": null,\n",
      "  \"ukn_token\": \"<|endoftext|>\",\n",
      "  \"sos_token\": \"<|endoftext|>\",\n",
      "  \"eos_token\": \"<|endoftext|>\",\n",
      "  \"mask_token\": null,\n",
      "  \"sub_word_prefix\": \"\",\n",
      "  \"use_sos_and_eos\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from utils.text import TextEncoder, get_symbols, default_french_encoder, get_encoder\n",
    "\n",
    "cleaners = [\n",
    "    {'name' : 'french_cleaners', 'to_lowercase' : False}\n",
    "]\n",
    "# Equivalent to :\n",
    "#cleaners = [\n",
    "#    'fr_convert_to_ascii',\n",
    "#    {'name' : 'expand_numbers', 'langue' : 'fr'},\n",
    "#    'collapse_whitespace'\n",
    "#]\n",
    "\n",
    "default = default_french_encoder()\n",
    "encoder = default_french_encoder(cleaners = cleaners, vocab_size = 148)\n",
    "gpt_encoder = get_encoder(lang = 'en', text_encoder = 'gpt2')\n",
    "print(default)\n",
    "print()\n",
    "print(encoder)\n",
    "print()\n",
    "print(gpt_encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save / load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Text encoder ==========\n",
      "Vocab (size = 148) : ['_', '-', '!', \"'\", '(', ')', ',', '.', ':', ';', '?', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l']\n",
      "Config : {\n",
      "  \"level\": 0,\n",
      "  \"lstrip\": false,\n",
      "  \"rstrip\": false,\n",
      "  \"cleaners\": [\n",
      "    {\n",
      "      \"name\": \"french_cleaners\",\n",
      "      \"to_lowercase\": false\n",
      "    }\n",
      "  ],\n",
      "  \"split_pattern\": null,\n",
      "  \"bpe_end_of_word\": null,\n",
      "  \"pad_token\": \"\",\n",
      "  \"sep_token\": null,\n",
      "  \"ukn_token\": null,\n",
      "  \"sos_token\": \"[SOS]\",\n",
      "  \"eos_token\": \"[EOS]\",\n",
      "  \"mask_token\": null,\n",
      "  \"sub_word_prefix\": \"\",\n",
      "  \"use_sos_and_eos\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "filename = 'example_data/example_text_encoder.json'\n",
    "\n",
    "encoder.save_to_file(filename)\n",
    "restored = TextEncoder.load_from_file(filename)\n",
    "\n",
    "print(restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use it to encode / decode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text : Bonjour  à Tous   !\n",
      "Cleaned text  : bonjour a tous !\n",
      "Encoded text  : [39 52 51 47 52 58 55 11 38 11 57 52 58 56 11  2]\n",
      "Decoded text  : bonjour a tous !\n"
     ]
    }
   ],
   "source": [
    "fr_text = \"Bonjour  à Tous   !\"\n",
    "\n",
    "cleaned = default.clean_text(fr_text)\n",
    "encoded = default.encode(fr_text)\n",
    "decoded = default.decode(encoded)\n",
    "\n",
    "print(\"Original text : {}\".format(fr_text))\n",
    "print(\"Cleaned text  : {}\".format(cleaned))\n",
    "print(\"Encoded text  : {}\".format(encoded))\n",
    "print(\"Decoded text  : {}\".format(decoded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage with tensorflow\n",
    "\n",
    "As the encoder uses dictionnary, string and regex function, it cannot be used in pure tensorflow functions (for graph optimization). \n",
    "\n",
    "The classical way to go around is to use `tf.py_function()` or `tf.numpy_function` like this :\n",
    "\n",
    "Note : this code comes from the `Tacotron2` data processing pipeline\n",
    "\n",
    "Note 2 : the `encode` and `decode` functions both handle `tf.Tensor`s (it simply convert them into `np.ndarray`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([39 52 51 47 52 58 55 11 38 11 57 52 58 56 11  2], shape=(16,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def encode_text(sentence):\n",
    "    return default.encode(sentence)\n",
    "\n",
    "@tf.function\n",
    "def encode(text):\n",
    "    encoded_text = tf.py_function(encode_text, [text], Tout = tf.int32)\n",
    "    encoded_text.set_shape([None])\n",
    "                \n",
    "    return encoded_text\n",
    "\n",
    "print(encode(fr_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
