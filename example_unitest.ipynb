{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699243c9",
   "metadata": {},
   "source": [
    "# Example unitest framework\n",
    "\n",
    "This is a custom unitest framework allowing to test *function's consistency*. \n",
    "\n",
    "For models' testing, it is not possible to get a clear output you could manually encode. Therefore, classical unitest frameworks are not enough to test this kind of models. However, it could be interesting to check, after an update, that you model still works correcly !\n",
    "\n",
    "The objective of this framework is to run a function that is *working* (does what you want) and dump its behavior on data. Later, it will load the saved output, performs again the function on the same input data and checks whether outputs match or not : if not, you have broken something :D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94437b57",
   "metadata": {},
   "source": [
    "## Simple example\n",
    "\n",
    "In the 1st cell, it is a simple example of comparison you can do on data types where there are 3 mistakes. As shown in the output, all tests are run and it has detected the 3 errors and tells you where they are (test name and position of the test (starting at index 0)).\n",
    "\n",
    "In the sencond cell, I have solved the 3 errors and re-run the tests : only failed tests are re-executed and now they all succeed !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c4b70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dict run test index 0\n",
      "test_dict run test index 1\n",
      "test_primitives run test index 0\n",
      "test_primitives run test index 1\n",
      "test_primitives run test index 2\n",
      "test_primitives run test index 3\n",
      "test_primitives run test index 4\n",
      "test_array run test index 0\n",
      "Tests summary :\n",
      "9 tests executed in 0.173 sec (6 passed)\n",
      "Failed tests (3) :\n",
      "\n",
      "===== test_dict =====\n",
      "3 tests executed in 0.000 sec (2 passed)\n",
      "Failed tests (1) :\n",
      "- Test 1 failed with message Invalid items (1) :\n",
      "Key c : '4' != '3'\n",
      "\n",
      "===== test_primitives =====\n",
      "5 tests executed in 0.000 sec (3 passed)\n",
      "Failed tests (2) :\n",
      "- Test 0 failed with message '1' != '2'\n",
      "- Test 3 failed with message 'True' != 'False'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from unitest import *\n",
    "\n",
    "@Test(sequential = True)\n",
    "def test_dict():\n",
    "    assert_equal({'a' : 1, 'b' : 2, 'c' : 3}, {'a' : 1, 'b' : 2, 'c' : 3})\n",
    "    assert_equal({'a' : 1, 'b' : 2, 'c' : 4}, {'a' : 1, 'b' : 2, 'c' : 3})\n",
    "    assert_not_equal({'a' : 1, 'b' : 2, 'c' : 3}, {'a' : 1, 'b' : 2, 'd' : 3})\n",
    "\n",
    "@Test\n",
    "def test_primitives():\n",
    "    assert_equal(1, 2)\n",
    "    assert_equal(True, 1)\n",
    "    assert_false(0)\n",
    "    assert_true(1 == 2)\n",
    "    assert_none(None)\n",
    "\n",
    "@Test\n",
    "def test_array():\n",
    "    assert_equal(np.arange(5), [0, 1, 2, 3, 4])\n",
    "\n",
    "res = run_tests(to_run = 'all', debug = True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ede3a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dict run test index 1\n",
      "test_primitives run test index 0\n",
      "test_primitives run test index 3\n",
      "Tests summary :\n",
      "9 tests executed in 0.173 sec (9 passed)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@Test\n",
    "def test_dict():\n",
    "    assert_equal({'a' : 1, 'b' : 2, 'c' : 3}, {'a' : 1, 'b' : 2, 'c' : 3})\n",
    "    assert_equal({'a' : 1, 'b' : 2, 'c' : 4}, {'a' : 1, 'b' : 2, 'c' : 4})\n",
    "    assert_not_equal({'a' : 1, 'b' : 2, 'c' : 3}, {'a' : 1, 'b' : 2, 'd' : 3})\n",
    "\n",
    "@Test\n",
    "def test_primitives():\n",
    "    assert_equal(1, 1)\n",
    "    assert_equal(True, 1)\n",
    "    assert_false(0)\n",
    "    assert_true(1 == 1)\n",
    "    assert_none(None)\n",
    "\n",
    "@Test\n",
    "def test_array():\n",
    "    assert_equal(np.arange(5), [0, 1, 2, 3, 4])\n",
    "\n",
    "res = run_tests(debug = True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b1f69c",
   "metadata": {},
   "source": [
    "## More realistic tests : image processing\n",
    "\n",
    "Following examples come from `test/test_utils_image.py` for image processing testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7cd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from unitest import Test, assert_function, assert_equal, assert_smaller\n",
    "\n",
    "from utils.image import load_image, augment_image\n",
    "from utils.image.mask_utils import create_color_mask\n",
    "from utils.image.box_utils import *\n",
    "\n",
    "_filename = os.path.join('test', '__datas', 'lena.jpg')\n",
    "\n",
    "_image = None\n",
    "\n",
    "def maybe_load_image():\n",
    "    global _image\n",
    "    if _image is None:\n",
    "        _image = load_image(_filename)\n",
    "    \n",
    "    return _image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea81504",
   "metadata": {},
   "source": [
    "### The lazy test.\n",
    "\n",
    "The 1st test has a hardcoded target (known in advance) and the value is a function (`get_image_size`) and additional arguments (`filename`) will be given to the function when executing this test.\n",
    "\n",
    "The 2nd one is *pure lazy test* where both target and value are calculated when executing the test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad121e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Test\n",
    "def image_io():\n",
    "    image = maybe_load_image()\n",
    "    \n",
    "    assert_equal((512, 512), get_image_size, _filename)\n",
    "    assert_equal(lambda: get_image_size(_filename), lambda: get_image_size(image))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c87bb5",
   "metadata": {},
   "source": [
    "### Random-dependant test\n",
    "\n",
    "As we could expect, data augmentation is random (random noise, random blur, ...) but it is still possible to test it even without knowing the expected output !\n",
    "\n",
    "The `assert_function` does not take *target* and *value* but a **function** as argument (in this case `augment_image`) with all `args` and `kwargs` passed to the function. \n",
    "\n",
    "When the test will be executed for the 1st time : \n",
    "1. Run the function with given parameters.\n",
    "2. Save the produced output in a file\n",
    "\n",
    "When the test will be re-executed later :\n",
    "1. Reload the saved output\n",
    "2. Run the function with given parameters.\n",
    "3. Compare loaded output with computed output.\n",
    "\n",
    "**Important Note** : it is necessary that your function is reproducible (with `seed`) otherwise you cannot reproduce an output ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88659979",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Test(contains_randomness = True)\n",
    "def test_image_augmentation():\n",
    "    image = maybe_load_image()\n",
    "    \n",
    "    for i, transform in enumerate(['color', 'flip_horizontal', 'flip_vertical', 'noise', 'hue', 'saturation', 'brightness', 'contrast']):\n",
    "        assert_function(augment_image, image, transform, 1., seed = i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68b1cc8",
   "metadata": {},
   "source": [
    "### Sequential test\n",
    "\n",
    "This test comes from `test/test_utils_text.py` for `TextEncoder` testing.\n",
    "\n",
    "This test also performs *function consistency* (as in the previous example) but with a specificity : it is sequential. It means that if a test craches, it will not execute the next ones. The logic behind this feature is that if the encoder failed to re-encode correctly the 1st sentence, it will also fail for next ones, no need to test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf06e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "_default_sentences  = [\n",
    "    \"Hello World !\",\n",
    "    \"Bonjour Ã  tous !\",\n",
    "    \"1, 2, 3, 4, 5, 6, 7, 8, 9 et 10 !\",\n",
    "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\\\n",
    "    Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure \\\n",
    "    dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident,\\\n",
    "    sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "]\n",
    "\n",
    "def test_text_encoder(encoder):\n",
    "    set_sequential()\n",
    "    for sent in _default_sentences:\n",
    "        assert_function(encoder.encode, sent)\n",
    "        assert_function(lambda text: encoder.decode(encoder.encode(text)), sent)\n",
    "        assert_function(encoder.split, sent, max_length = 150)\n",
    "    \n",
    "    assert_function(encoder.join, * _default_sentences)\n",
    "\n",
    "@Test\n",
    "def test_english_text_encoder():\n",
    "    from utils.text import default_english_encoder\n",
    "    \n",
    "    test_text_encoder(default_english_encoder())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "765248ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests summary :\n",
      "34 tests executed in 0.626 sec (34 passed)\n",
      "\n",
      "All tests succeed !\n"
     ]
    }
   ],
   "source": [
    "res = run_tests()\n",
    "print(res)\n",
    "res.assert_succeed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b18bac",
   "metadata": {},
   "source": [
    "## Run all tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75307bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import *\n",
    "from unitest import run_tests\n",
    "\n",
    "run_tests(debug = True).assert_succeed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433514e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
